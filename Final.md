### Laurie Mazza
### Final Project
My starting point of inspiration for this project came from the idea of being able to throw my phone and using the resulting output to create a visual. It was after playing around with OSChook that I realized throwing the phone does not give as interesting as an output as I had hoped and that continuously moving the phone produced more interesting results. Reaction-diffusion is a simulation that I personally like to watch as I find it almost zen-like to see it grow and change over time. This is what drove me to pick this simulation to work with. I wanted to have the user control this simulation in some way and thought throwing a phone fit this simulation well. Every time that I want to throw my phone I am in the exact opposite state from a zen state, making it clash with the feeling reaction-diffusion gives me. It was this clashing feeling that I was originally going for in this piece. After I had set this as my goal, I had discovered the disappointing output that throwing the phone gave me and had to rethink my goal for this project. Throughout the development of this project my technical goal remained the same as I wanted to control a simulation using the movement of my phone. While the experience that I wanted the user to have changed as the project changed, the main feeling of having created something of their own remained the same.
### Simulation Only Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/sabPRfXESmo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Output used for the simulation 
Below are photos of the OSChook readings that were drivers over the time of the simulation.


### Process
I started this project by focusing on using a reading output from OSChook to drive a Reaction-Diffusion simulation. I began using the first value from the rotation vector to control the f value. This value required some cleanup as it can go into the negative values which, along with some other values, can end up killing the simulation. Testing and adjusting gave a result that allowed for control of the simulation without the risk of killing it. After this, I moved on to trying to have the color of the simulation be live video from the webcam. While I was able to get the video to work and have the input impact it in some way, I could not get it to work with the simulation in the way that I want it. After focusing on that issue for longer than I should have, I decided to change my plan and have another input from the phone to control the color of the simulation. In order to do this, I tested various outputs from OSChook and different ways of manipulating them into usable numbers. Based on these results, I decided to use the first three values of the rotation vector to control the RGB values. This allows the user to simply rotate the phone to change the color. Since the color was using the rotation vector, I decided to change what controlled the f value to avoid direct overlap and give the user the ability to control them more independently of each other. I went with the light value as putting the phone in the light made the simulation grow more, similar to putting a plant in the light. This value had to be adjusted for use as its raw value was unusable for the simulation. The last thing I added in was gravity controlling the direction of the blur to allow for it to be manipulated in some way. After receiving feedback, I decided to make changes which resulted in the testing of controlling the k value along with the f value and which outputs were controlling what. In the end, this resulted in the f value no longer being controlled and the k value being controlled by the x-axis linear acceleration. This allows for an increase in the rate by simply shaking the phone side to side. The light sensor is still utilized by having it control the amount of the blur.

### Code
[Repository](https://github.com/LaurieAMazza/IMGD420X-Final)

### Video of Simulation and Controller
Having problems with this one. Will be up ASAP

### Feedback
The feedback that I received in class was that it took too long to grow and it seemed like it was a lack of control when using the phone to control the simulation. Based on this feedback I made adjustments to the simulation and what was controlling it. I speed up the growth by having the user control the k value rather than the f value and no longer linking it to the light sensor as the readings vary a lot based on the kind of lighting a user has access to. Since I found playing around with the light sensor to be interesting to experiment with, I decided to keep using it by having it control the rate of the blur. After I made these changes, I found that it was possible to play around with movement to develop new shapes and patterns within the simulation. I tested these changes out with a friend by not telling them what controlled what but by simply telling them to try moving the phone around. After testing out moving the phone, they tried to push the movements they noticed making changes to the extreme, resulting in new shapes forming. In the end, they said it was tiring but fun and found the shapes resulting from movement to feel like they were playing with a medium of art. Based on the feedback and the changes over the development of this project, I am unsure if I met my artistic goal but, I am happy that the end result was something that people can have fun playing around with. 